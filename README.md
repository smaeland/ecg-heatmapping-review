# ecg-heatmapping-review

Run and compare heatmapping methods on synthetic, median ECG data. The data are 150 000 normal and close-to-normal ECG generated by Vajira's [GAN model](https://www.nature.com/articles/s41598-021-01295-2), and processed using GE Muse 12SL to get 12-lead median samples. Since the latter is commercial software, these data are not published along with the GAN model, but the starting point is still the same as the data available on the GAN webpage. 

We predict (separately) the heart rate, the PR, QT and QRS intervals, and the amplitude of the T peak, R peak, and J-point elevation (STJ). For the three amplitudes, the amplitude is computed on lead V5.

The ML model is a convolutional neural network with residual connections, architechturally identical to the one used in Steven's [ECG paper](https://www.nature.com/articles/s41598-021-90285-5), better known just as the "StevenNet" model. We train one network for each prediction target, so 7 in total. 

Once the models are trained, heatmaps are computed using the following methods:
 - saliency ([API docs](https://captum.ai/api/saliency.html))
 - deepLIFT ([API docs](https://captum.ai/api/deep_lift.html))
 - smoothgrad ([API docs](https://captum.ai/api/noise_tunnel.html))
 - input * gradient ([API docs](https://captum.ai/api/input_x_gradient.html))
 - integrated gradients ([API docs](https://captum.ai/api/integrated_gradients.html))
 - deconvolution ([API docs](https://captum.ai/api/deconvolution.html))
 - guided gradCAM ([API docs](https://captum.ai/api/guided_grad_cam.html))
 - guided backprop ([API docs](https://captum.ai/api/guided_backprop.html))
 - gradient SHAP ([API docs](https://captum.ai/api/gradient_shap.html))
 - random heatmap

The last one is not a method per se, but just creates a random heatmap to serve as a baseline that all methids should be able to beat. 
All these methods are provided by the Captum library, and are documented in the links above, and also described in more detail here: [Algorithm descriptions](https://captum.ai/docs/attribution_algorithms).

> Note: Many of the methods are by default set to compute the results and then multiply it by the input (e.g. `DeepLift(model, multiply_by_inputs=True, ...)`), which is the basis of the input*gradients method. To avoid mixing methods, we use `multiply_by_inputs=True` for all methods that support it.

In addition, layer-wise relevance propagation (LRP, [API docs](https://captum.ai/api/lrp.html)) is implemented, but does not seem to do anything useful. Feel free to experiment with this one.

For evaluation of the methods, the first step is to compute the heatmaps for a representative set of single ECGs, and compare them visually. This requires some medical experience. The second step is without a human in the loop, where we for each ECG samples, iteratively
 - use the heatmap to select the most important data point
 - randomise the data at this point
 - run the CNN again and get the prediction error (compared to the true value)
 - observe how the prediction error increases for each iteration
 - go to the second-most important data point, and repeat 
We call this the "perturbarion procedure". See also the overleaf document for a more thorough explanation.

To aid the visual comparison where we have 7 targets times 9 heatmapping methods times many ECG samples = a gazillion plots, a separate webpage is set up to show them in a grid: https://smaeland.github.io/ecg-heatmapping-webpage/

Below follows the steps to run the entire analysis. The instructions assume that we are working on eX3, but it will of course work elsewhere too.


## Check out the code 
```bash
mkdir ecg-heatmapping && cd ecg-heatmapping
git clone git@github.com:smaeland/ecg-heatmapping-review.git
```

## Data
On eX3, copy the data files in HDF5 format from this location
```bash
mkdir data && cd data
cp -r /home/steffen/D1/projects/ecg/data/ECG_8lead_median_Run4_hdf5 . 
cd ..
``` 
CSV files also available: 
```
/home/steffen/D1/projects/ecg/data/ECG\ 8-lead\ \(csv\)\ median\ and\ rhythm\ Run4/ # median ECG 
/home/steffen/D1/projects/ecg/data/pulse2pulse_150k_ground_truth.csv  # sample info
```

## Environment
The code relies on PyTorch and the [Captum](https://captum.ai/) XAI library.

### First-time setup
```bash
module load pytorch-py37-cuda11.2-gcc8/1.9.1    # Load pytorch
virtualenv --system-site-packages venv          # Create a virtual environment, using the just-loaded pytorch module
source venv/bin/activate                        # Activate it
pip install -r requirements.txt                 # Install Captum (and other packages)
```

### Load existing setup
```bash
module load pytorch-py37-cuda11.2-gcc8/1.9.1 
source venv/bin/activate
```

## Usage

If starting from CSV files, convert median data and sample info to HDF5:
```bash
python convert_csv_to_hdf5.py
```
Otherwise, skip this step.

Train StevenNet model locally:
```bash
cd ecg-heatmapping-review
mkdir models
python train_medians.py -target qt -output_name_tag stevennet
``` 
Possible targets are `VentRate`, `qt`, `pr`, `qrs`, `STJ_v5`, `T_PeakAmpl_v5`, `R_PeakAmpl_v5`. `output_name_tag` is a name of you own choosing, that wil be appended to the model save file. 
Note that for the amplitudes and for PR, some samples might not have a true value, and has to be skipped. In this case there will be a "pre-check" which takes a while to run, before the actual training starts.

To train on the batch system:
```bash
sbatch train_model.sh
```
In the `train_model.sh` file, uncomment the lines corresponding to the model you wish to train.

**_Technical note:_** If running on the HGX2 or the A100 nodes, there is a missing library needed by pytorch. If you are not able to run pytorch on these nodes, copy over the missing library manually:
```bash
cp -r /home/steffen/D1/projects/ecg/ecg-heatmapping-review/lib .
``` 

Compute model performance metrics for the newly trained models: Edit `evaluate_model.py` so that `model_savefile_mapping` points to the correct save files, then run
```bash
python evaluate_model.py
# or
sbatch evaluate_model_metrics.sh
```

Get a list of events from test dataset with complete metadata
```bash
python events_for_plotting.py
```

Now we can create a heatmap for the given target observable, and heatmapping method. Edit `heatmap.py` so that `model_save_file_mapping1` points to the correct save file for each target. There are several savefile mappings defined, to allow for quick testing of different models. Maybe not the most elegant approach, but okay. The `--model 1` option selects models from `model_save_file_mapping1`, while specifying `--model 2` would select models from `model_save_file_mapping2`, and so on. 

```bash
mkdir plots 
python heatmaps.py --observable qt --method saliency --model 1 --event_index 0-10
```
Description of available options:
 - `-ei / --event_index` option specifies which test events to make plots for. Use `--event_index 0` to plot only the first ECG sample (we use zero-indexing), or `--event_index 0-10` to plot all samples from 0 to 10.
 - `-p / --no_show`: Don't open plots in a new window
 - `-s / --save`: Save the plots as PDF and PNG. Filenames are created on basis of input options
 - `-mc / merge_channels`: Select between
   - `none`: Show all 12 leads separately with separate heatmaps 
   - `average`: Plot lead V5, with an average heatmap of all channels overlaid
   - `v5_only`: Plot only lead V5, with heatmap corresponding only to lead V5
 - `-od / output_dir`: Where to save the plots, default "plots"


Remember that to be able to see the plots, you must have an X server running (i.e. have connected to eX3 using `ssh -Y`). Otherwise, save the plots and open them afterwards. 

Create all heatmaps for all methods and observables on the batch system:
```
sbatch create_heatmap_plots.sh
```

Run the region perturbation procedure, for an objective evaluation of heatmaps. This computes results and saves them to the `error_analysis` directory, but does not create the final plots.
```
python compare_heatmaps.py compute_errors --observable qt --num_iterations 300 --event_limit 1000 --output_dir error_analysis
```

Run region perturbation for all observables on the batch system:
```
sbatch compare_heatmaps.py
```

Create the comparison plots, using results generated above:
```
python compare_heatmaps.py plot_comparison --observable qt --input_dir error_analysis --output_dir plots
```
