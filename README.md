# ecg-heatmapping-review

Run and compare heatmapping methods on synthetic, median ECG data. The data are 150 000 normal and close-to-normal ECG generated by Vajira's [GAN model](https://www.nature.com/articles/s41598-021-01295-2), and processed using GE Muse 12SL to get 12-lead median samples. Since the latter is commercial software, these data are not published along with the GAN model, but the starting point is still the same as the data available on the GAN webpage. 

We predict (separately) the heart rate, the PR, QT and QRS intervals, and the amplitude of the T peak, R peak, and J-point elevation (STJ). For the three amplitudes, the amplitude is computed on lead V5.

The ML model is a convolutional neural network with residual connections, architechturally identical to the one used in Steven's [ECG paper](https://www.nature.com/articles/s41598-021-90285-5), better known just as the "StevenNet" model. We train one network for each prediction target, so 7 in total. 

Once the models are trained, heatmaps are computed using the following methods:
 - saliency ([API docs](https://captum.ai/api/saliency.html))
 - deepLIFT ([API docs](https://captum.ai/api/deep_lift.html))
 - smoothgrad ([API docs](https://captum.ai/api/noise_tunnel.html))
 - input * gradient ([API docs](https://captum.ai/api/input_x_gradient.html))
 - integrated gradients ([API docs](https://captum.ai/api/integrated_gradients.html))
 - deconvolution ([API docs](https://captum.ai/api/deconvolution.html))
 - guided gradCAM ([API docs](https://captum.ai/api/guided_grad_cam.html))
 - guided backprop ([API docs](https://captum.ai/api/guided_backprop.html))
 - gradient SHAP ([API docs](https://captum.ai/api/gradient_shap.html))
 - random heatmap

The last one is not a method per se, but just creates a random heatmap to serve as a baseline that all methids should be able to beat. 
All these methods are provided by the Captum library, and are documented in the links above, and also described in more detail here: [Algorithm descriptions](https://captum.ai/docs/attribution_algorithms).

> Note: Many of the methods are by default set to compute the results and then multiply it by the input (e.g. `DeepLift(model, multiply_by_inputs=True, ...)`), which is the basis of the input*gradients method. To avoid mixing methods, we use `multiply_by_inputs=True` for all methods that support it.

For evaluation of the methods, the first step is to compute the heatmaps for a representative set of single ECGs, and compare them visually. This requires some medical experience. The second step is without a human in the loop, where we for each ECG samples, iteratively
 - use the heatmap to select the most important data point
 - randomise the data at this point
 - run the CNN again and get the prediction error (compared to the true value)
 - observe how the prediction error increases for each iteration
 - go to the second-most important data point, and repeat 
We call this the "perturbarion procedure". See also the overleaf document for a more thorough explanation.

To aid the visual comparison where we have 7 targets times 9 heatmapping methods times many ECG samples = a gazillion plots, a separate webpage is set up to show them in a grid: https://smaeland.github.io/ecg-heatmapping-webpage/

Below follows the steps to run the entire analysis. The instructions assume that we are working on eX3, but it will of course work elsewhere too.


## Check out the code 
```bash
mkdir ecg-heatmapping && cd ecg-heatmapping
git clone git@github.com:smaeland/ecg-heatmapping-review.git
```


## Data
On eX3, copy the data files in HDF5 format from this location
```
mkdir data && cd data
cp -r /home/steffen/D1/projects/ecg/data/ECG_8lead_median_Run4_hdf5 . 
cd ..
``` 
CSV files also available: 
```
/home/steffen/D1/projects/ecg/data/ECG\ 8-lead\ \(csv\)\ median\ and\ rhythm\ Run4/ # median ECG 
/home/steffen/D1/projects/ecg/data/pulse2pulse_150k_ground_truth.csv  # sample info
```

## Environment
The code relies on PyTorch and the [Captum](https://captum.ai/) XAI library.

### First-time setup
```bash
module load pytorch-py37-cuda11.2-gcc8/1.9.1    # Load pytorch
virtualenv --system-site-packages venv          # Create a virtual environment, using the just-loaded pytorch module
source venv/bin/activate                        # Activate it
pip install -r requirements.txt                 # Install Captum (and other packages)
```

### Load existing setup
```bash
module load pytorch-py37-cuda11.2-gcc8/1.9.1 
source venv/bin/activate
```

## Usage

If starting from CSV files, convert median data and sample info to HDF5:
```
python convert_csv_to_hdf5.py
```
Otherwise, skip this step.

Train StevenNet model:
```
python train_medians.py -target qt -output_name_tag stevennet
# or
sbatch train_model.sh
```

Compute model performance metrics
```
python evaluate_model.py
# or
sbatch evluate_model_metrics.sh
```

Get a list of events from test dataset with complete metadata
```
python events_for_plotting.py
```

Create a heatmap
```
python heatmaps.py --observable qt --method saliency --model 1 --event_index 0-10
```

Create all heatmaps for all methods and observables
```
sbatch create_heatmap_plots.sh
```

Run region perturbation procedure for objective evaluation of heatmaps
```
python compare_heatmaps.py compute_errors --observable qt --num_iterations 300 --event_limit 1000 --output_dir error_analysis
```

Run region perturbation for all observables
```
sbatch compare_heatmaps.py
```

Plot results from above
```
python compare_heatmaps.py plot_comparison --observable qt --input_dir error_analysis --output_dir plots
```
